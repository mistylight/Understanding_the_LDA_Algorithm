{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Training\n",
    "<figure>\n",
    "<img src=https://s2.loli.net/2022/02/28/X7vzOlDHJtP6UnM.png style=\"width: 600px\">\n",
    "<figcaption>The LDA training algorithm from <a href=http://www.arbylon.net/publications/text-est.pdf>Parameter estimation for text analysis</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "from types import SimpleNamespace\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_docs: 7241 num_topics: 10 num_words: 4989\n"
     ]
    }
   ],
   "source": [
    "# === corpus loading ===\n",
    "class NeurIPSCorpus:\n",
    "    def __init__(self, data_path, num_topics, max_num_words=10000, max_doc_length=1000):\n",
    "        self.docs = []\n",
    "        self.word2id = OrderedDict()\n",
    "\n",
    "        word2cnt = defaultdict(int)\n",
    "        with open(data_path) as fin:\n",
    "            for line in fin:\n",
    "                for word in line.strip().split():\n",
    "                    word2cnt[word] += 1\n",
    "        \n",
    "        word2cnt = sorted(list(word2cnt.items()), key=lambda x: x[1], reverse=True)\n",
    "        if len(word2cnt) > max_num_words:\n",
    "            word2cnt = word2cnt[:max_num_words]\n",
    "        word2cnt = dict(word2cnt)\n",
    "\n",
    "        with open(data_path) as fin:\n",
    "            for line in fin:\n",
    "                doc = []\n",
    "                for word in line.strip().split():\n",
    "                    if len(doc) >= max_doc_length: break\n",
    "                    if word not in word2cnt: continue\n",
    "                    if word not in self.word2id: \n",
    "                        self.word2id[word] = len(self.word2id)\n",
    "                    doc.append(self.word2id[word])\n",
    "                self.docs.append(doc)\n",
    "\n",
    "        self.num_docs = len(self.docs)\n",
    "        self.num_topics = num_topics\n",
    "        self.num_words = len(self.word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        print(\n",
    "            \"num_docs:\", self.num_docs, \n",
    "            \"num_topics:\", self.num_topics, \n",
    "            \"num_words:\", self.num_words\n",
    "        )\n",
    "        import json\n",
    "        state_dict = {\n",
    "            \"docs\": self.docs,\n",
    "            \"word2id\": self.word2id,\n",
    "            \"num_topics\": self.num_topics,\n",
    "        }\n",
    "        json.dump(state_dict, open(\"data/papers.json\", \"w\"))\n",
    "\n",
    "corpus = NeurIPSCorpus(\n",
    "    data_path=\"data/papers.txt\", \n",
    "    num_topics=10,\n",
    "    max_num_words=5000,\n",
    "    max_doc_length=1000,\n",
    ")\n",
    "hparams = SimpleNamespace(\n",
    "    alpha=np.ones([corpus.num_topics], dtype=float) / corpus.num_topics,\n",
    "    beta = np.ones([corpus.num_words], dtype=float) / corpus.num_topics,\n",
    "    gibbs_sampling_max_iters=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "from types import SimpleNamespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# === corpus loading ===\n",
    "cdef class NeurIPSCorpus:\n",
    "    cdef list docs\n",
    "    cdef dict word2id\n",
    "    cdef int num_docs\n",
    "    cdef int num_topics\n",
    "    cdef int num_words\n",
    "\n",
    "    def __init__(self, state_dict_path):\n",
    "        cdef dict state_dict = json.load(open(state_dict_path))\n",
    "        self.docs = state_dict[\"docs\"]\n",
    "        self.word2id = state_dict[\"word2id\"]\n",
    "        self.num_topics = state_dict[\"num_topics\"]\n",
    "        self.num_docs = len(self.docs)\n",
    "        self.num_words = len(self.word2id)\n",
    "        print(\n",
    "            \"num_docs:\", self.num_docs, \n",
    "            \"num_topics:\", self.num_topics, \n",
    "            \"num_words:\", self.num_words\n",
    "        )\n",
    "\n",
    "cdef class Hyperparameters:\n",
    "    cdef np.ndarray alpha\n",
    "    cdef np.ndarray beta\n",
    "    cdef int gibbs_sampling_max_iters\n",
    "    \n",
    "    def __init__(self, alpha, beta, gibbs_sampling_max_iters):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta \n",
    "        self.gibbs_sampling_max_iters = gibbs_sampling_max_iters\n",
    "\n",
    "cdef NeurIPSCorpus corpus = NeurIPSCorpus(\"data/papers.json\")\n",
    "cdef Hyperparameters hparams = Hyperparameters(\n",
    "    alpha=np.ones([corpus.num_topics], dtype=float) / corpus.num_topics,\n",
    "    beta=np.ones([corpus.num_words], dtype=float) / corpus.num_topics,\n",
    "    gibbs_sampling_max_iters=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === initialization ===\n",
    "print(\"Initializing...\", flush=True)\n",
    "cdef np.ndarray n_doc_topic = np.zeros([corpus.num_docs, corpus.num_topics], dtype=float) # n_m^(k)\n",
    "cdef np.ndarray n_topic_word = np.zeros([corpus.num_topics, corpus.num_words], dtype=float) # n_k^(t)\n",
    "cdef np.ndarray z_doc_word = np.zeroes([corpus.num_docs, corpus.max_doc_length], dtype=int)\n",
    "\n",
    "cdef int topic_ij\n",
    "for doc_i in tqdm(range(corpus.num_docs)):\n",
    "    for j, word_j in enumerate(corpus.docs[doc_i]):\n",
    "        topic_ij = random.randint(0, corpus.num_topics - 1)\n",
    "        n_doc_topic[doc_i, topic_ij] += 1\n",
    "        n_topic_word[topic_ij, word_j] += 1\n",
    "        z_doc_word[doc_i, j] = topic_ij\n",
    "\n",
    "# === Gibbs sampling ===\n",
    "print(\"Gibbs sampling...\", flush=True)\n",
    "for iteration in range(hparams.gibbs_sampling_max_iters):\n",
    "    for doc_i in tqdm(range(corpus.num_docs)):\n",
    "        for j, word_j in enumerate(corpus.docs[doc_i]):\n",
    "            # remove the old assignment\n",
    "            topic_ij = z_doc_word[doc_i, j]\n",
    "            n_doc_topic[doc_i, topic_ij] -= 1\n",
    "            n_topic_word[topic_ij, word_j] -= 1\n",
    "            # compute the new assignment\n",
    "            p_doc_topic = (n_doc_topic[doc_i, :] + hparams.alpha) \\\n",
    "                        / np.sum(n_doc_topic[doc_i] + hparams.alpha)\n",
    "            p_topic_word = (n_topic_word[:, word_j] + hparams.beta[word_j]) \\\n",
    "                        / np.sum(n_topic_word + hparams.beta, axis=1)\n",
    "            p_topic = p_doc_topic * p_topic_word\n",
    "            p_topic /= np.sum(p_topic)\n",
    "            # record the new assignment\n",
    "            new_topic_ij = np.random.choice(np.arange(corpus.num_topics), p=p_topic)\n",
    "            n_doc_topic[doc_i, new_topic_ij] += 1\n",
    "            n_topic_word[new_topic_ij, word_j] += 1\n",
    "            z_doc_word[doc_i, j] = new_topic_ij\n",
    "\n",
    "    # === Check convergence and read out parameters ===\n",
    "    print(f\"Iter [{iteration}] ===\", flush=True)\n",
    "    theta = (n_doc_topic + hparams.alpha) / np.sum(n_doc_topic + hparams.alpha, axis=1, keepdims=True)\n",
    "    phi = (n_topic_word + hparams.beta) / np.sum(n_topic_word + hparams.beta, axis=1, keepdims=True)\n",
    "    print(\"theta:\", theta, \"phi:\", phi, flush=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b98ce01d6443f9ed8de6b2298de45a5462ef793670bc2920aea9ee85010384a1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
